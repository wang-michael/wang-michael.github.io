---
layout:     post
title:      天池首届阿里云POLARDB数据性能大赛参赛记录
date:       2018-12-3
author:     W-M
header-img: img/post-bg-coffee.jpeg
catalog: true
tags:
    - 竞赛
---
>本文记录我最近一段时间参加天池首届POLARDB数据性能大赛踩的一些坑和想方设法解决这些坑时的一些收获。         

_ _ _
### **前言**
第一次做这种性能优化类的比赛，支持C++与Java两种语言，众多参赛选手使用C++作为参赛语言不同，我使用的是Java，初赛30/1653,晋级200支队伍，复赛34/200。  
<img src="/img/2018-12-3/rank.png" width="300" height="300" alt="初赛架构图" />
<center>比赛排名</center>  

由于比赛经验不足，没有针对比赛的数据集做仔细的分析，复赛感觉非常遗憾，本来可以做的更好...  不管怎么说，通过这次比赛还是学到了很多东西，下面就来记录下我的解题过程。    

### **赛题描述**
比赛总体分成了初赛和复赛两个阶段，整体要求实现一个简化、高效的 kv 存储引擎。  

初赛要求支持 Write、Read 接口。  
```java
public abstract void write(byte[] key, byte[] value);
public abstract byte[] read(byte[] key);
```
复赛在初赛题目基础上，还需要额外实现一个 Range 接口。
```java
public abstract void range(byte[] lower, byte[] upper, AbstractVisitor visitor);
```
程序评测逻辑 分为2个阶段：   

1）Recover 正确性评测：
* 此阶段评测程序会并发写入特定数据（key 8B、value 4KB）同时进行任意次 kill -9 来模拟进程意外退出（参赛引擎需要保证进程意外退出时数据持久化不丢失），接着重新打开 DB，调用 Read、Range 接口来进行正确性校验

2）性能评测

* 随机写入：64 个线程并发随机写入，每个线程使用 Write 各写 100 万次随机数据（key 8B、value 4KB）
* 随机读取：64 个线程并发随机读取，每个线程各使用 Read 读取 100 万次随机数据
* 顺序读取：64 个线程并发顺序读取，每个线程各使用 Range 有序（增序）遍历全量数据 2 次

注：
2.2 阶段会对所有读取的 kv 校验是否匹配，如不通过则终止，评测失败；
2.3 阶段除了对迭代出来每条的 kv校 验是否匹配外，还会额外校验是否严格字典序递增，如不通过则终止，评测失败。
语言限定：C++ & JAVA，一起排名 java内存占用限制：3G(评测程序占用500M，实际可用2.5G)；磁盘占用限制：320G

### **初赛过程**
#### **解题过程**
涉及文件IO的操作，以4K为单位的顺序读写都是最优秀的([别让SSD成半吊子 4K对齐很关键](http://ssd.zol.com.cn/537/5374950_all.html))。评测程序的value也正是4KB，所以开始我采用了最朴素的思想，顺序写，跳读：  
* 写的时候
  1. 将value放入专门存Value的文件中，获得FilePosition;
  2. 将Key和FilePosition一起存入专门存Key-FilePosition的文件中。
* 读的时候
  1. 将key-FilePosition文件中的数据加载到内存构建索引
  2. 根据key对应的FilePosition从Value文件中获取Value，返回给调用方

思路定下来了之后，实现时不可避免的会遇到许多问题：  
1. 写的时候如何保证kill -9数据不丢失
2. 由于是64线程每个线程写100w条数据，并发写肯定要比串行写快，那么如何做到并发写呢？？？  
3. 内存中的索引选什么数据结构呢？这个数据结构真的能在3G内存下装满6400w的key-filePosition吗？

对于第一个问题，肯定不能通过每次写入数据都刷盘之后才返回这种方式来解决，否则肯定效率很低。实际使用的是mmap写文件的方式，通过把数据写入mmap方式映射的MappedByteBuffer中，数据就会写入到os的pageCache中，由os负责定期将数据刷新到硬盘，这样在进程被kill掉时就可以做到已经写入的数据不丢失。使用mmap还有一个好处，就是由于内核空间地址与用户空间的虚拟地址映射到同一个物理地址，每次mmap读写操作相比于普通的read、write操作减少了一次系统调用以及内核缓冲区与用户缓冲区之间数据相互拷贝的开销。  

下面就是解决第二个问题：如何做到并发写。我使用了64个线程每个线程写自己的数据文件和索引文件的形式，各个线程之间的写操作互不影响，做到了完全无锁的写入。由于评测程序写入的key是有重复的，并且题目要求对于后写入的key对应的value要覆盖前面写入的key对应的value，所以我在key-fileposition文件中记录了key写入的时间戳，将key-fileposition文件中读入索引中时，key如果相同就比较其时间戳，新的时间戳对应的value覆盖旧的value。  

对于第三个问题：选用什么数据结构。HashMap肯定是对应于这个使用场景的最直观的的解决方案(复赛的时候又尝试了其它的解决方案，后面会介绍)，我要存入到索引中信息包含6400w个key-value对，每个key-value对对应于两个基本类型中的long，16个字节，6400w * 16B = 976.5625M，但是要知道java中原生的HashMap存储的是对象而不是基本类型，对象是由对象头、实例数据、对齐填充三部分组成的([一个Java对象到底占用多大内存](https://www.cnblogs.com/magialmoon/p/3757767.html))，在64位机器上，未开启指针压缩的情况下Long类型对象占用的内存为24byte，如果将其放到HashMap中占用的额外内存会更多(比如HashMap将其封装到了一个Node节点中，Node节点中存储的是此Long对象的引用，这个对象引用在64位机器上也要占用8字节，还有HashMap为了维护自身数据结构所需要占用的一些内存空间)，这样的话我们就会发现使用java原生的HashMap肯定是不行的，很自然的就想到了有没有可使用的支持基本类型的HashMap库，在网上参考了这两篇文章([大型HashMap评估：JDK、FastUtil、Goldman Sachs、HPPC、Koloboke与Trove](http://www.importnew.com/13687.html),[应用JMH测试大型HashMap的性能](https://mp.weixin.qq.com/s/WN50R5alQg1ATl8yWlmm9A))，本地又做了一些测试之后，最终决定使用HPPC的LongLongHashMap。  

到这里为止，初赛的思路就基本定下来了：    
* 随机写入阶段：64个线程分别写自己的数据文件与索引文件，做到了无锁的完全并发，最后会生成64个数据文件与64个索引文件。数据文件中仅存储value，索引文件中存储的是一个key-value的map，map的key值是一个long型，即为写入的key值；map的value也是一个long型，其中存储的信息包含三部分，前6位存储key对应的4kb的value在哪个文件中存储，之后的38位存储key写入的时间戳，最后的20位存储4kb的value在文件中的存储位置。 
* 索引加载阶段：单线程顺序读取64个索引文件，将64个索引文件中所有的key-value对放在内存中的一个LongLongHashMap中存储，根据key写入的时间戳确定key之间的覆盖关系，解决了重复key需要覆盖的问题
* 随机读取阶段：根据指定的要读取的key在内存中的LongLongHashMap找寻其对应的value，确定其在文件中的存储位置后读取出来将其返回

根据上述思路实现了初版代码之后又经过了很多次的优化，确定最优参数组合，初赛的时间达到了247s左右。这时榜上第一名的时间大概在220s左右，虽然参加比赛的大部分同学使用的都是cpp，但是我觉得语言上的差距导致的时间差不会有这么大，于是继续寻找思路上是否存在问题。  

在上述初赛思路的三个阶段中，随机写入阶段和随机读取阶段都做到了无锁完全并发，优化空间不大了，但索引加载阶段是单线程的，这里应该还有优化空间，很自然的就想到将索引加载阶段由单线程变为多线程的，我们之前确定索引加载阶段使用单线程的原因是HPPC的LongLongHashMap是线程不安全的，所以为了实现上的简单就先将所有的key-fileposition写到了同一个LongLongHashMap中，那么能不能做到将key-fileposition写到多个HashMap中呢？  

要使得key-fileposition写到多个HashMap中，要解决的关键问题就是写入到多个HashMap中的数据要均匀，否则就会导致下面的问题：
1. 不均匀的写的话可能导致有的LongLongHashMap的数据过多，有的LongLongHashMap的数据过少，这样分成多个HashMap的意义就不大了
2. 加载索引到多个HashMap的过程中要锁冲突带来的性能损失较大(不均匀的写会导致多个线程写同一个HashMap的几率较大，这样的话锁冲突就很多)

**要使数据均匀的分布到多个HashMap中，就要求我选择通用性的散列算法，在数据不均匀分布的情况下也能将数据均匀的散列到多个HashMap中去**(在这里我假设评测程序写入的数据是不均匀分布的，实际上评测数据在6400w数据写入时写入的数据是非常均匀的，这是我复赛最大的坑，最遗憾的地方，要不然复赛结果会好的多，后面记录复赛的时候会详述。。)。**在寻找满足上述要求的散列算法的时候我想到Jdk1.7中ConcurrentHashMap将数据散列到多个Segment使用的Wang/Jenkins算法，这个算法有以下的意义：**  
1. 对元素的hashCode进行了一次Object.hashCode()基础之上的再散列。之所以进行再散列，目的是减少散列冲突，使元素能够均匀的分布在不同的Segment上，从而提高容器的存取效率。假如散列的质量差到极点，那么所有的元素都在一个Segment中，不仅存取元素缓慢，分段锁也会失去意义。  
2. 再hash使得hashCode中每一位的值都可能成为确定Segment所在位置的值，让数字的每一位都参加到散列运算当中，从而减少散列冲突。

可见这个算法对于我这里的使用场景是非常适合的，于是我就采用了这个算法将数据散列到多个HashMap中去，这里HashMap的个数可配置，散列代码如下：  
```java
private int hash(Object k) {
    int h = 0;

    h ^= k.hashCode();

    // Spread bits to regularize both segment and index locations,
    // using variant of single-word Wang/Jenkins hash.
    h += (h <<  15) ^ 0xffffcd7d;
    h ^= (h >>> 10);
    h += (h <<   3);
    h ^= (h >>>  6);
    h += (h <<   2) + (h << 14);
    return h ^ (h >>> 16);
}

public int getWhichHashMap(Long key) {
    int h = hash(key);
    // hash到64个map中
//        int whichHashMap = h >>> 26;

    // hash到128个map中
//        int whichHashMap = h >>> 25;

    // hash到256个map中
//        int whichHashMap = h >>> 24;

    // hash到512个map中,返回值在0~511之间
    int whichHashMap = h >>> 23;

//        int whichHashMap = h >>> 22;
    return whichHashMap;
}
```
经过线上性能测试之后，使用512个HashMap性能最好，使用了这种加载索引的方式，我们的总用时比之前提升了10多s，最终时间定格在235.02s。与第一名的差距在15s左右，与第10名的差距在10s左右。   

<img src="/img/2018-12-3/polarfirstcompetition.png" width="700" height="700" alt="初赛架构图" />
<center>图1：初赛架构图</center>   

#### **初赛还可以做但是没有想到的优化**
1、使用Direct IO   
<img src="/img/2018-12-3/directIo.png" width="500" height="500" alt="DirectIO" />
<center>图2：DirectIO</center>   
复赛需要自己管理缓存，才了解到DirectIO，它并不是 JAVA 原生支持的IO方式，但可以通过 JNA/JNI 调用 native 方法做到。从上图我们可以看到 ：Direct IO 绕过了 PageCache，使用 fileChannel.read() 可能触发 PageCache 的预读IO，但是在随机读这种场景下我们其实并不希望操作系统帮我们干太多事，因为你帮我预读了我也用不上，下一个要读的数据在预读的数据中的可能性非常小。  

使用DirectIO还有一个优势，数据可以直接从BlockLayer层加载到用户空间中，省去了从pageCache中加载到用户空间中的内存拷贝。

Java 目前原生并不支持DIO，但也有好心人封装好了 Java 的 JNA 库，实现了 Java 的 Direct IO，github 地址：https://github.com/smacke/jaydio  

复赛中将初赛随机读部分改为使用DirectIO之后，随机读性能又提升了大概5s左右。如果初赛能加上这个优化，可以达到230s左右。  

2、写文件时的mmap参数是否达到最优？？？  
我在写的时候是64个线程并发写64个数据文件，采用MappedByteBuffer的方式，每次映射多大的Buffer才能达到最优呢?每次映射的Buffer越大，map与unmap操作占用的时间就比较长，就越容易造成64个线程并发映射时的阻塞等待，多线程写的优势不能完全发挥；每次映射的Buffer越小 ，map与unmap的次数就越多，也可能会造成占用时间的增加，所以应该是存在一个最优的Buffer参数的。  

我这边测试是每个线程每次映射1M的Buffer时速度是最快的，但是赛后和cpp选手交流发现他们有的是每次映射16K最快，有的是每次映射128K最快，不知道为什么差距会这么大。  

### **复赛过程**
#### **解题过程**  
赛题要求64个线程Range两次全量的数据，限时1h，也就是说如果不做缓存，需要128 * 6400w次的io，这肯定是不行的，所以复赛一开始我就开始考虑缓存的实现。开始的思路是：由于Range的过程需要保证遍历的数据严格递增，所以肯定要对写入的6400w的key进行排序，之后每次从尚未遍历过的最小的key开始加载一段key对应的value到内存中的cache，然后64个range线程遍历内存中的cache，遍历完成之后加载下一段cache到内存中，重复这个操作直到完成。  

初始思路实现时需要解决的两个问题：  
1. 如何尽快对6400w数据进行排序
2. 如何尽快的将指定的一段key对应的value加载到cache中

对于6400w数据进行排序的问题，由于我将数据散列到了512个key[],value[]数组中(复赛在range阶段使用key-value数组代替了HashMap)，所以就是要将512个key[]数组合并为一个全局有序的key数组，开始采用的方式是先64线程对512个key数组进行快排，每个线程负责快排8个key数组；然后构建一个512个元素的最小堆，堆中的元素来源于这512个key数组，每次从堆中取出一个最小的元素，之后判断这个元素来源于哪一个数组，再从这个key数组中取出一个新的元素加入到堆中。单线程从堆中取数据重复6400w次之后即得到一个全局有序的数组。这种排序方式在快排阶段是64线程并发的，但是在从堆中取数据的过程是单线程进行的，最终耗时19s左右。  

之后我想到能不能把单线程从堆中取数据的过程改为多线程对快排后512个有序的key[]数组进行归并，于是使用Java中的Fork-Join框架实现了多线程归并512个有序的key[]数组，使用这种方式之后将排序的时间缩短到了3-4s。这种实现方式的劣势在于需要占用多余空间(800M左右)，而且这段空间在占用短时间之后就会被废弃，这里占用的额外空间导致了我之后将缓存放到堆外时经常会出现CGGroup OOM错误，jvm堆内存参数比较难控制。  

对于尽快将指定的一段key对应的value加载到cache中的问题，每次加载65536个key对应的value到内存中，占用的空间就在256M左右，由于我采用的还是初赛时的散列方式，每次要加载的一段key中的每个key对应的value都可能位于64个文件中的任意一个文件，所以这里没法使用顺序读，只能使用随机读。对于随机读，适当的并发(并发线程数可配置)肯定要比单线程要快的，所以我将每个大段的key分为了多个小段(比如64个小段，每个小段内有65536/64条数据)，每个小段对应的value都由一个单独的线程来加载到内存，使用了这样的思路实现的第一版代码时间在2300s左右，和我的预计非常不相符，按照这样的思路来说本来range阶段的时间应该比随机读阶段时间 * 2多一些，总时间应该在6,7百秒左右，为什么会这样？  

经过仔细排查之后，发现64个线程读取cache的时间竟然和填充cache的时间竟然差不多，这肯定是不合理的，最后发现问题出在GC上，一个将long转换为byte[]的函数，这个函数当时的写法是这样的：  
```java
public static byte[] longToByte(long number) {
    byte[] result = new byte[8];
    result[0] = (byte) (number >> 56);
    result[1] = (byte) (number >> 48);
    result[2] = (byte) (number >> 40);
    result[3] = (byte) (number >> 32);
    result[4] = (byte) (number >> 24);
    result[5] = (byte) (number >> 16);
    result[6] = (byte) (number >> 8);
    result[7] = (byte) (number);
    return result;
}
```    
这个函数每次调用都会new一个新的byte数组，这个数组对象占用的堆空间大小为16(对象头，开启指针压缩的情况下) + 8(实例数据) + 0(对齐填充，不需要对齐) = 24字节，对象不大，但架不住函数调用的次数多，每次遍历一块cache，函数都要被调用65536 * 64次，产生的朝生夕灭的result对象占用的空间就达到了96M，range 6400w数据需要填充977次cache，这样就会产生非常多的朝生夕灭的result对象，GC次数非常多，之后我将这个函数改成了下面这样：  
```java
public static byte[] longToByte(long number, byte[] result) {
    result[0] = (byte) (number >> 56);
    result[1] = (byte) (number >> 48);
    result[2] = (byte) (number >> 40);
    result[3] = (byte) (number >> 32);
    result[4] = (byte) (number >> 24);
    result[5] = (byte) (number >> 16);
    result[6] = (byte) (number >> 8);
    result[7] = (byte) (number);
    return result;
}
```
时间由2300s左右一下降到了663s，可见这里的影响还是非常大的。  

初始思路实现完了之后就是优化了，反思上面的初始思路，发现在填充cache阶段与读取cache阶段是串行的，而这两个阶段应该是可以并行起来的，方式就是使用两个buffer，填充线程cache填充一块buffer的同时，读取线程读取另一块buffer，遵循这样的优化思路，实现了一个版本，时间到了543s左右，进步了100多s。  

543s版本的填充cache的线程数是64个，读取cache的线程数也是64个，填充cache与读取cache是同时进行的，仔细想想这样的线程数分配是不合理的，填充cache的线程属于io线程，我希望在io线程完成一次io操作之后能够尽快获得cpu，这样想来io操作的线程数是要比读取线程数多一些的，在io线程与读取线程同时进行操作的时候，io线程获得cpu的几率就会更大些。之后经过参数优化测试，发现将填充cache的线程数设置为96个是最为合理的。  

再之后做的就是一些小优化，比如调整GC参数设置新生代与老年代的比例到最优：  
* young 区过大，对象在年轻代待得太久，多次在两个survivor区之间进行拷贝，并且由于我的程序内存中大对象比较多，可能造成有的大对象由于老年代没有连续空间存放它，一直留存在新生代中，这样在新生代进行minor GC完成存活对象标记后，老年代内存担保失败，转为full gc，耗时增多
* old 区过小，会频繁触发 old 区的 cms gc

最后经过性能测试，我的程序jvm参数NewRatio参数设为3，即新生代与老年代比为1:3，效果最佳。还有做的一些优化就是IO方面的小优化，比如使用unsafe进行内存拷贝操作之类的，经过这些优化之后程序用时到了460s左右。    

#### **复赛还可以做但是没有想到的优化**
1、 最重要的优化没有做-顺序读，最大的遗憾！还有下面的优化都是基于顺序读的思想可以做的一些优化。  
做完上述这些优化之后，复赛时间已经过半了，榜上第一名成绩在415s左右，我猜测肯定还有大的优化没有做，因为我现在虽然加上了缓存，但是填充缓存的过程使用的是随机读磁盘，所以在之后的时间里也有想过怎么才能做到顺序读磁盘，但是没有想到很好的解决方案。直到比赛结束的前一天，看到有的选手讨论说**虽然正确性测试阶段数据分布不是很均匀，但是性能测试阶段的数据分布是均匀的**，我恍然大悟，我使用的分段算法是通用的散列算法，而排名较为靠前的选手采用的分段方式是利用key的高n位进行分段，比如n为10，那么就会分为1024段，每段数据对应的文件大小就是256M左右，**这种分段方式相比我的分段方式最大的好处在于可以实现顺序读，因为段之间是有序的，段内是无序的**，排序的时候只需要对每个段进行排序，排完之后的数据自然就做到了全局有序，在读取的时候可以一次性把整个段内的数据使用顺序读加载到内存的cache中，然后再使用内存中的索引对cache进行随机读取。  
2、 顺序读的时候也要使用多buf，每个buf对应于一个填充buf的线程。如果只使用一个大的buf的话，那么就是填充cache与读取cache是串行的；使用多个buf，可以做到填充buf与读取buf并行，但是这里具体使用多少个buf并行对磁盘进行顺序读可以把磁盘io打满就需要线上测试了，由于我并没有实现，所以没有发言权，但是有java选手实现了这种方式，并且通过线上测试发现4个buf时性能最好。  
<img src="/img/2018-12-3/rangejiagou.png" width="700" height="700" alt="range" />
<center>图3：比赛后一名java大佬选手分享的自己的上述思想的架构图(f1-f4代表4个读盘线程)</center> 

3、 对于这种需要实现程序自己管理cache的场景，采用DIO方式顺序读比较好，可以省去从PageCache复制到应用程序内存的开销。  
4、 填充多buf的线程要比visit线程的优先级高，给io线程分配更多的时间片，具体参见java大佬的博客[POLARDB数据库性能大赛JAVA选手分享-14减少线程切换,15绑核](https://lexburner.github.io/)       
5、 使用堆外内存：上面那篇博客中java大佬自己实现了java DIO的堆外内存读写，实际上jayDio库中也有将数据读取到堆外内存的API，即DirectIOByteChannel以及AlignedByteBuffer。使用堆外内存的好处在于对GC友好，将几块大的Cache放入堆外内存应该可以减少GC耗时。  
6、 对于随机读阶段，因为内存空间有剩余，在随机读阶段可以先顺序读取几段数据到cache中，在随机读之前先判断此key对应的数据是否在cache中，不在cache中时再去磁盘上去取实际数据。  

其实我觉得上述的优化思路除了第4条涉及到的知识点我确实不知道，是赛后其它选手科普的，其它几条如果我能想到数据均匀分布的话这些优化应该都是可以做的，由于比赛经验不足，仅仅测试过正确性检测阶段的数据分布不是很均匀就认为性能测试阶段的6400w数据分布也不均匀，导致上述的一系列优化都没有做，非常遗憾！！！ 但或许这也就是比赛的魅力所在吧，在过程中学到了很多，期待以后还能有机会，有时间参与到这样的比赛中来！！！   

参考的几位竞赛的大佬分享的文章：  
[POLARDB数据库性能大赛JAVA选手分享](https://lexburner.github.io/)  
[第一届阿里云PolarDB数据库性能大赛初赛分析](https://mp.weixin.qq.com/s?__biz=MzIxMTE2MzA0Mg==&mid=2650799205&idx=1&sn=cf8944df9494147b800914e186a51aea&chksm=8f52dea8b82557beff32eca59b8d50956cd2fd85212b5f9ffcd80d1a6cd364a59a03772d8ea2&mpshare=1&scene=1&srcid=1211vcvJx4u7YZxLbLyXTel3#rd)  






      